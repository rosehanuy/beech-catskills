{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from geocube.api.core import make_geocube\n",
    "from scipy.ndimage import generic_filter, gaussian_filter, convolve\n",
    "from shapely import Point, box\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStack():\n",
    "    \"\"\"Class containing processed data for input into minimum noise fraction\"\"\"\n",
    "    def __init__(self,root,year,site_name,epsg):\n",
    "        self.root = root\n",
    "        os.makedirs(self.root / 'sentinel_data' / site_name,exist_ok=True)\n",
    "        self.folder = site_name\n",
    "        self.year = year\n",
    "        self.site_name = site_name\n",
    "        self.epsg = int(epsg)\n",
    "\n",
    "        b = xr.open_dataarray(self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}.nc')\n",
    "        self.input_data = b.rio.write_crs(self.epsg).rio.set_spatial_dims(x_dim=\"x\",y_dim=\"y\",).rio.write_coordinate_system()\n",
    "\n",
    "        self.y_coords = self.input_data.y.values\n",
    "        self.x_coords = self.input_data.x.values\n",
    "\n",
    "        self.season_data = None\n",
    "\n",
    "        f1 = self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_aligned_bands.nc'\n",
    "        if os.path.isfile(f1):\n",
    "            self.aligned_data = xr.open_dataarray(f1)\n",
    "        else:\n",
    "            self.aligned_data = None \n",
    "\n",
    "        f2 = self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_variables.nc'\n",
    "        if os.path.isfile(f2):\n",
    "            self.all_variables = xr.open_dataarray(f2)\n",
    "        else:\n",
    "            self.all_variables = None\n",
    "\n",
    "        f3 = self.root / 'output' / self.site_name.upper() / f'basal_area_{self.site_name.upper()}.nc'\n",
    "        if os.path.isfile(f3):\n",
    "            aa = xr.open_dataset(f3)\n",
    "            self.basal_area = aa    # dataset containing basal area/categories for each species\n",
    "        else:\n",
    "            self.basal_area = None\n",
    "            \n",
    "        \n",
    "        f4 = self.root / 'output' / self.site_name.upper() / f'{self.site_name.upper()}_endmember_beech.gpkg'\n",
    "        if os.path.isfile(f4):\n",
    "            self.endmember_manual = gpd.read_file(f4)\n",
    "        else:\n",
    "            self.endmember_manual = None\n",
    "        \n",
    "        self.labels = [os.path.join(self.root / 'output' / self.site_name.upper(),x) for x in os.listdir(self.root / 'output' / self.site_name.upper()) if x.endswith('labels.csv')]\n",
    "        \n",
    "        self.diffs = None\n",
    "\n",
    "        self.mnf_input = None\n",
    "\n",
    "    def plot_input_data(self,band):\n",
    "        self.input_data.isel(band=band).plot(col='time',col_wrap=4,robust=True)\n",
    "\n",
    "    def plot_season_data(self,band):\n",
    "        if self.season_data is not None:\n",
    "            self.season_data.isel(band=band).plot(col='time',col_wrap=4,robust=True)\n",
    "        else:\n",
    "            print('need to run method: select_season_data()')\n",
    "    \n",
    "    def plot_aligned_data(self,band):\n",
    "        if self.aligned_data is not None:\n",
    "            self.aligned_data.isel(band=band).plot(col='time',col_wrap=4,robust=True)\n",
    "        else:\n",
    "            print('need to run method: coregister_data()')\n",
    "    \n",
    "    def select_season_data(self,date_list): # input date_list based on manual inspection of input data\n",
    "        e = [str(x) for x in self.input_data.time.values if any(date in str(x) for date in date_list)]\n",
    "        self.season_data =  self.input_data.sel(time=e)\n",
    "    \n",
    "    def get_gradient(self,im) :\n",
    "        # Calculate the x and y gradients using Sobel operator\n",
    "        grad_x = cv2.Sobel(im,cv2.CV_32F,1,0,ksize=3)\n",
    "        grad_y = cv2.Sobel(im,cv2.CV_32F,0,1,ksize=3)\n",
    "        # Combine the two gradients\n",
    "        grad = cv2.addWeighted(np.absolute(grad_x), 0.5, np.absolute(grad_y), 0.5, 0)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def coregister_data(self):\n",
    "    \n",
    "        # replace nas with 0\n",
    "        b_sel = self.season_data.where(~np.isnan(self.season_data), other=0)\n",
    "        # create reference image: mean of all temporal steps\n",
    "        reference_image = self.season_data.mean(dim=['band','time'])\n",
    "        # replace na with 0 \n",
    "        reference_image = reference_image.where(~np.isnan(reference_image), other=0)\n",
    "\n",
    "        # convert input data to numpy float32 numpy arrays\n",
    "        reference_image = np.float32(reference_image.to_numpy())\n",
    "        im = np.float32(b_sel.to_numpy())\n",
    "\n",
    "        # define dimensions for output image\n",
    "        height = b_sel.shape[2]\n",
    "        width = b_sel.shape[3]\n",
    "        time = b_sel.shape[0]\n",
    "        band = b_sel.shape[1]\n",
    "\n",
    "        ## Define motion model\n",
    "        warp_mode = cv2.MOTION_AFFINE\n",
    "        # Set the warp matrix to identity matrix\n",
    "        warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
    "        # Set the stopping criteria for the algorithm.\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 5000,  1e-10)\n",
    "\n",
    "        # Create empty array of correct size for new aligned images\n",
    "        im_aligned = np.zeros((time,band,height,width), dtype=np.float32 )\n",
    "\n",
    "        # loop over time and band dimensions and apply coregistration to each band\n",
    "        for i in range(0,time):\n",
    "            for j in tqdm(range(0,band)) :\n",
    "                (_, warp_matrix) = cv2.findTransformECC(self.get_gradient(reference_image), self.get_gradient(im[i,j,:,:]),warp_matrix, warp_mode, criteria)    \n",
    "                                                                       \n",
    "                im_aligned[i,j,:,:] = cv2.warpAffine(im[i,j,:,:], warp_matrix, (width,height), flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "\n",
    "        b_align = xr.DataArray(im_aligned, \n",
    "                      coords={'time':b_sel.time,'band': b_sel.band,'y': b_sel.y,'x':b_sel.x}, \n",
    "                      dims=['time','band','y','x'])\n",
    "        b_align = b_align.where(b_align!=0, other=np.nan) # reset 0 values to na\n",
    "    \n",
    "\n",
    "        self.aligned_data = b_align\n",
    "        b_align.to_netcdf(self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_aligned_bands.nc')\n",
    "        print(f'coregisterd bands saves to {self.root} / sentinel_data / {self.folder} / {self.year}_{self.site_name}_aligned_bands.nc')\n",
    "\n",
    "    \n",
    "    def remove_outliers(self,a,norm=True):\n",
    "        a = a.where(np.isfinite(a),np.nan)\n",
    "        if norm == True:\n",
    "            a = a.where((a >= -1) & (a <= 1))\n",
    "        else:\n",
    "            a = a.where((a >= 0) & (a <= 8))\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def add_all_variables(self,num_months,num_of_peak_evi_month): # number of months\n",
    "        if self.aligned_data is None:\n",
    "            print('need to run method: coregister_data()')\n",
    "        else:\n",
    "        # separate all bands onto 1 dimension\n",
    "            t = np.arange(0,num_months)\n",
    "            season_list = [f'month{x}' for x in t]\n",
    "\n",
    "            array_list1 = list()\n",
    "            for i, season in enumerate(season_list):\n",
    "                t = self.aligned_data.isel(time=i).reset_coords('time',drop=True)\n",
    "                t = t.assign_coords({'band':[f'{x}_{season}' for x in self.aligned_data.band.values]})\n",
    "                array_list1.append(t)\n",
    "\n",
    "            m1 = xr.concat(array_list1,dim='band')\n",
    "\n",
    "            # calculate evi, lswi, slavi, psri for each season \n",
    "            #season_list = ['month1','month2','month3', etc....]\n",
    "            array_list2 = list()\n",
    "            for i in range(0,num_months):\n",
    "                blue = array_list1[i].sel(band=f'B02_{season_list[i]}')\n",
    "                green = array_list1[i].sel(band=f'B03_{season_list[i]}')\n",
    "                red = array_list1[i].sel(band=f'B04_{season_list[i]}')\n",
    "                nir = array_list1[i].sel(band=f'B8A_{season_list[i]}')\n",
    "                sw1 = array_list1[i].sel(band=f'B11_{season_list[i]}')\n",
    "                sw2 = array_list1[i].sel(band=f'B12_{season_list[i]}')\n",
    "                re2 = array_list1[i].sel(band=f'B06_{season_list[i]}')\n",
    "\n",
    "\n",
    "                evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1)).expand_dims({'band':[f'evi_{season_list[i]}']}) # range -1 - 1\n",
    "                evi = self.remove_outliers(evi)\n",
    "\n",
    "                lswi = (nir - sw1)/(nir + sw1).expand_dims({'band':[f'lswi_{season_list[i]}']}) # range -1 to 1\n",
    "                lswi = self.remove_outliers(lswi)\n",
    "\n",
    "                slavi = nir/(red + sw2).expand_dims({'band':[f'slavi_{season_list[i]}']}) # range 0 - 8\n",
    "                slavi = self.remove_outliers(slavi,norm=False)\n",
    "\n",
    "                psri = (red - blue)/re2.expand_dims({'band':[f'psri_{season_list[i]}']}) # range -1 to 1\n",
    "                psri = self.remove_outliers(psri)\n",
    "\n",
    "                bright = (0.3510*blue)+(0.3813*green)+(0.3437*red)+(0.7196*nir)+(0.2396*sw1)+(0.1949*sw2).expand_dims({'band':[f'bright_{season_list[i]}']})\n",
    "                bright = self.remove_outliers(bright)\n",
    "\n",
    "                wet = (0.2578*blue)+(0.2305*green)+(0.0883*red)+(0.1071*nir)+(-0.7611*sw1)+(-0.5308*sw2).expand_dims({'band':[f'wet_{season_list[i]}']})\n",
    "                wet = self.remove_outliers(wet)\n",
    "\n",
    "                green = (-0.3599*blue)+(-0.3533*green)+(-0.4734*red)+(0.6633*nir)+(0.0087*sw1)+(-0.2856*sw2).expand_dims({'band':[f'green_{season_list[i]}']})\n",
    "                green = self.remove_outliers(green)\n",
    "\n",
    "                season_vi = xr.concat([evi,lswi,slavi,psri,bright,wet,green],dim='band')\n",
    "\n",
    "                array_list2.append(season_vi)\n",
    "\n",
    "            m2 = xr.concat(array_list2,dim='band')\n",
    "\n",
    "            m3 = xr.concat([m1,m2],dim='band')\n",
    "\n",
    "            ## mask non-forest pixels\n",
    "            self.all_variables = m3.where(m3.sel(band=f'evi_month{num_of_peak_evi_month}')>0.5,other=np.nan)\n",
    "            self.all_variables = m3\n",
    "            # save\n",
    "            self.all_variables.to_netcdf(self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_variables.nc')\n",
    "            print(f'variables saved to {self.root}/sentinel_data/{self.folder}/{self.year}_{self.site_name}_variables.nc')\n",
    "\n",
    "\n",
    "\n",
    "    # calculate differences for indices of given two months\n",
    "    def get_season_differences(self,first,second,desc):\n",
    "        array_list = list()\n",
    "        for vi in ['evi', 'lswi', 'slavi','psri','wet','bright','green']:\n",
    "            d = self.all_variables.sel(band=f'{vi}_{first}') - self.all_variables.sel(band=f'{vi}_{second}')\n",
    "            d = d.expand_dims({'band':[f'{vi}_{first}_{second}_diff']})\n",
    "            array_list.append(d)\n",
    "        \n",
    "        self.diffs = xr.concat(array_list,dim='band')\n",
    "        self.diffs.to_netcdf(self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_diffs_{desc}.nc')\n",
    "        print(f'diffs saved to {self.root}/sentinel_data/{self.folder}/{self.year}_{self.site_name}_diffs_{desc}.nc')\n",
    "\n",
    "\n",
    "    def run_pca(self,n,monthstouse,bandstouse,diff_desc,output_desc,save=False):\n",
    "        \n",
    "        e = [x for x in self.all_variables.band.values if (any(y in x for y in monthstouse) & any(z in x for z in bandstouse))]\n",
    "        a1 = self.all_variables.sel(band=e)\n",
    "\n",
    "        d = xr.open_dataarray({self.root} / 'sentinel_data' / {self.folder} / f'{self.year}_{self.site_name}_diffs_{diff_desc}.nc')\n",
    "        \n",
    "        f = [x for x in d.band.values if any(y in x for y in ['bright','green','wet'])]\n",
    "        d = d.sel(band=f)\n",
    "        a2 = xr.concat([a1,d],dim='band')\n",
    "        a = a2.values \n",
    "\n",
    "        a_2d = rearrange(a, 'c h w -> (h w) c')\n",
    "        # drop all na from data\n",
    "        a2d_dropna = a_2d[~np.isnan(a_2d).any(axis=1)]\n",
    "        # get indices of dropped nas to replace them later\n",
    "        na_indices = np.where(np.isnan(a_2d))\n",
    "        row_indices = np.unique(na_indices[0]) # indices of rows that were dropped from a_2d\n",
    "\n",
    "        # rescale data around approximate mean of 0\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(a2d_dropna)\n",
    "\n",
    "        \n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(data_scaled)\n",
    "        pca_data = pca.transform(data_scaled)\n",
    "        print(f'explained variance: {np.cumsum(pca.explained_variance_ratio_)}')\n",
    "        \n",
    "            \n",
    "        # replace nans that were dropped earlier\n",
    "        output_array = np.full((a_2d.shape[0],pca_data.shape[1]), np.nan)  # create array of correct shape\n",
    "\n",
    "        # Fill in the rows that were not NaN\n",
    "        non_nan_indices = np.setdiff1d(np.arange(a_2d.shape[0]), row_indices)  # get indices that don't have na values\n",
    "        output_array[non_nan_indices] = pca_data\n",
    "\n",
    "        # # reshape to original dimensions\n",
    "        height = self.all_variables.shape[1]\n",
    "        width = self.all_variables.shape[2]\n",
    "        new_pca_data = rearrange(output_array, '(h w) c -> c h w', h=height, w=width)\n",
    "        pca_array = xr.DataArray(new_pca_data, \n",
    "                            coords={'band': np.arange(0,n),'y': self.all_variables.y,'x': self.all_variables.x}, \n",
    "                            dims=['band','y','x'])\n",
    "        # todo: return object with explained variance, plot function etc.\n",
    "        if save is True:\n",
    "            pca_array.to_netcdf(self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_pca_{output_desc}.nc')\n",
    "            print(f'pca saved to {self.root}/sentinel_data/{self.folder}/{self.year}_{self.site_name}_pca_{output_desc}.nc')\n",
    "        \n",
    "        return pca, pca_array\n",
    "\n",
    "\n",
    "    # def plot_pca(self,rows,n_comps):\n",
    "    #     fig = plt.subplots(figsize=(20,5))\n",
    "    #     for i in range(0,n_comps):\n",
    "    #         plt.subplot(rows, n_comps, i+1)\n",
    "    #         self.pca.isel(band=i).plot()\n",
    "    #     plt.show()\n",
    "\n",
    "    def make_mnf_input(self, v1, v2,month,diff_desc):\n",
    "        \"\"\"\n",
    "        Assemble given version for input to MNF(), assuming existence of saved requisite .nc files\n",
    "\n",
    "        v1 = 'gs' or 'ssf' (growing season or spring-summmer-fall)\n",
    "        v2 = 'tc' or 'nontc' (tasseled cap or non tasseled cap)\n",
    "        month = 'month0, month1.....monthn' specifies month for which to include all bands and indices\n",
    "        diff_desc = which diff version to use\n",
    "\n",
    "        \"\"\"\n",
    "        # select specified month\n",
    "        if len(month) == 1:\n",
    "            e = [x for x in self.all_variables.band.values if month[0] in x]\n",
    "            a = self.all_variables.sel(band=e)\n",
    "        else:\n",
    "            e = [x for x in self.all_variables.band.values if (any(y in x for y in month))]\n",
    "            a = self.all_variables.sel(band=e)\n",
    "        \n",
    "        if v2 == 'tc':\n",
    "            f = [x for x in a.band.values if any(y in x for y in ['B','evi','bright','green','wet'])]\n",
    "            a = a.sel(band=f)\n",
    "        else:\n",
    "            f = [x for x in a.band.values if any(y in x for y in ['B','evi','lswi','slavi','psri'])]\n",
    "            a = a.sel(band=f)\n",
    "\n",
    "        pca = xr.open_dataarray(self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_pca_{v1}_{v2}.nc')\n",
    "        pca = pca.isel(band=slice(0,10))\n",
    "\n",
    "        b = xr.concat([a,pca],dim='band')\n",
    "\n",
    "        if diff_desc is not None:\n",
    "            d = xr.open_dataarray(self.root / 'sentinel_data' / self.folder / f'{self.year}_{self.site_name}_diffs_{diff_desc}.nc')\n",
    "\n",
    "            if v2 == 'tc':\n",
    "                f = [x for x in d.band.values if any(y in x for y in ['evi','bright','green','wet'])]\n",
    "                d = d.sel(band=f)\n",
    "            else:\n",
    "                f = [x for x in d.band.values if any(y in x for y in ['evi','lswi','slavi','psri'])]\n",
    "                d = d.sel(band=f)\n",
    "\n",
    "            b = xr.concat([a,d,pca],dim='band')\n",
    "        self.mnf_input = b\n",
    "\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version\n",
    "class MNF():\n",
    "    \"\"\"class to create mnf image and extract endmember spectra based on basal area\"\"\"\n",
    "    def __init__(self,input_data):\n",
    "        self.basal_area = input_data.basal_area\n",
    "        self.y_coords = input_data.input_data.y.values\n",
    "        self.x_coords = input_data.input_data.x.values\n",
    "        self.data = input_data.input_data.values  # shape = (band, y, x) \n",
    "\n",
    "        self.mnf = None\n",
    "\n",
    "        self.endmember_coords = None\n",
    "        self.endmember_spectra = None\n",
    "        self.endmember_mean = None\n",
    "\n",
    "    def make_2d_input_data(self):\n",
    "        x_2d = rearrange(self.data,'c h w -> (h w) c')\n",
    "        x_dropna = x_2d[~np.isnan(x_2d).any(axis=1)]  # Remove rows with NaNs\n",
    "        \n",
    "        # get indices of dropped nas to replace them later\n",
    "        na_indices = np.where(np.isnan(x_2d))\n",
    "        row_indices = np.unique(na_indices[0]) # indices of rows that were dropped from a_2d\n",
    "\n",
    "        return x_2d, x_dropna, row_indices\n",
    "\n",
    "\n",
    "\n",
    "    def get_noise_region(self,percentile):\n",
    "        image = self.data\n",
    "        # apply 7x7 window, calculate variance\n",
    "        variance_map = np.mean([generic_filter(image[b,:, :], lambda x: np.var(x), size=7) for b in range(image.shape[0])], axis=0)\n",
    "        # find areas with variance < 30th percentile\n",
    "        threshold = np.percentile(np.nan_to_num(variance_map,np.nanmean(variance_map)), percentile)  # Select the lowest 25% variance regions\n",
    "        low_var = variance_map < threshold\n",
    "\n",
    "        # extract largest region of low variance\n",
    "        binary_mask = low_var.astype(np.uint8)\n",
    "\n",
    "        # Find contours of homogeneous regions\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # Get the largest region\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        # Get bounding box coordinates\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        # Extract the subregion from the image\n",
    "        return image[:,y:y+h, x:x+w]\n",
    "\n",
    "    # calculate sqrt of inverse covariance matrix of noise matrix\n",
    "    def get_sqrt_inv_cov(self,x):  \n",
    "        (vals, V) = np.linalg.eig(x)\n",
    "        SRV = np.diag(1. / np.sqrt(vals))\n",
    "        return V @ SRV @ V.T\n",
    " \n",
    "        \n",
    "    def plot_mnf(self,rows, cols, n_components):\n",
    "        if self.mnf is None:\n",
    "            print('need to run make_mnf()')\n",
    "        else:\n",
    "\n",
    "            fig = plt.subplots(figsize=(15,8))\n",
    "\n",
    "            for i in range(0,n_components):\n",
    "                plt.subplot(rows, cols, i+1)\n",
    "                plt.imshow(self.mnf[i,:,:])\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    def get_ba_threshold(self):\n",
    "        r = self.basal_area.values\n",
    "        r = r[~np.isnan(r)]\n",
    "\n",
    "        return round(np.percentile(r,99),3)\n",
    "\n",
    "    def get_endmembers(self):  # return pixels with > 99th percentile of basal area\n",
    "        thresh = self.get_ba_threshold()\n",
    "        h = self.basal_area\n",
    "        rows, cols = np.where(h>=thresh)\n",
    "        pixels = [(int(row),int(col)) for row,col in zip(rows,cols)]\n",
    "        print(f'found {len(pixels)} beech pixels with greater than {thresh} basal area')\n",
    "        self.endmember_coords = pixels\n",
    "\n",
    "        p_list = []\n",
    "\n",
    "        for i in range(0,len(self.endmember_coords)):\n",
    "            p_list.append(self.mnf[:,self.endmember_coords[i][0],self.endmember_coords[i][1]])\n",
    "\n",
    "        p_array = np.array(p_list) \n",
    "        self.endmember_spectra = p_array\n",
    "\n",
    "        p_mean = np.mean(p_array,axis=0)\n",
    "        self.endmember_mean = p_mean\n",
    "\n",
    "    # plot endmembers in mnf feature space\n",
    "    def plot_feature_space(self):\n",
    "        t = self.mnf[0:2,:,:]\n",
    "        t = rearrange(t,'c h w -> c (h w)')\n",
    "        t2 = self.endmember_spectra\n",
    "\n",
    "        fig,ax = plt.subplots()\n",
    "\n",
    "        plt.scatter(t[0,:],t[1,:])\n",
    "        plt.scatter(t2[:,0],t2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_data = DataStack(root,2019,'bart',26919)\n",
    "\n",
    "#roi1.plot_input_data(band=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually inspect input data and select the timestamps to keep\n",
    "roi1.select_season_data(date_list=['05-08','06-12','07-07','08-26','09-20','10-15'])\n",
    "roi1.plot_season_data(band=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi1.coregister_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables saved to c:\\Users\\roseh\\OneDrive - Hunter - CUNY\\Documents\\beech_tree/sentinel_data/bartlett/2019_bart_variables.nc\n"
     ]
    }
   ],
   "source": [
    "roi1.add_all_variables(num_months=6,num_of_peak_evi_month=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffs saved to c:\\Users\\roseh\\OneDrive - Hunter - CUNY\\Documents\\beech_tree/sentinel_data/bartlett/2019_bart_diffs_summerminusfall_nonmasked.nc\n"
     ]
    }
   ],
   "source": [
    "roi1.get_season_differences('month2','month5','summerminusfall_nonmasked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance: [0.60511863 0.79735553 0.8480642  0.8904002  0.916802   0.9403233\n",
      " 0.9509751  0.95836663 0.96463484 0.96983236 0.97449994 0.9779058\n",
      " 0.981031   0.983791   0.9861946 ]\n",
      "pca saved to c:\\Users\\roseh\\OneDrive - Hunter - CUNY\\Documents\\beech_tree/sentinel_data/bartlett/2019_bart_pca_gs_tc.nc\n"
     ]
    }
   ],
   "source": [
    "gs_months=['month1','month2','month3']\n",
    "ssf_months=['month0','month2','month5']\n",
    "\n",
    "tc_bands = ['B','evi','bright','green','wet']\n",
    "nontc_bands = ['B','evi','lswi','slavi','psri']\n",
    "\n",
    "\n",
    "\n",
    "p1, p2 = roi1.run_pca(n=15,monthstouse=gs_months,bandstouse=tc_bands,diff_desc='summerminusfall',output_desc='gs_tc',save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Noise Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNF():\n",
    "    def __init__(self,mnf_image,noise_bbox,noise_cov_matrix,reg_noise_cov_matrix,whitening_matrix,basal_area,labels,root,site_name):\n",
    "        self.mnf_image = mnf_image\n",
    "\n",
    "        self.noise_bbox = noise_bbox\n",
    "        self.noise_cov_matrix = noise_cov_matrix\n",
    "        self.reg_noise_cov_matrix = reg_noise_cov_matrix\n",
    "        self.whitening_matrix = whitening_matrix\n",
    "        self.basal_area = basal_area\n",
    "        self.labels = labels\n",
    "        self.root = root\n",
    "        self.site_name = site_name\n",
    "        \n",
    "    \n",
    "    def plot_noise_region(self):\n",
    "        bx, by = self.noise_bbox.exterior.xy\n",
    "\n",
    "        plt.imshow(self.mnf_image[0,:,:])\n",
    "        plt.fill(bx,by,edgecolor='red',facecolor='red',alpha=0.4)\n",
    "    \n",
    "    def check_noise_cov_matrix(self):\n",
    "        # check stability of noise covariance matrix\n",
    "        determinant = np.linalg.det(self.noise_cov_matrix) # should be greater than 10e-10\n",
    "        condition = np.linalg.cond(self.noise_cov_matrix) # should be below 10e7\n",
    "\n",
    "        eigvals = np.linalg.eigvals(self.noise_cov_matrix)\n",
    "\n",
    "        print(f'original matrix\\ndeterminant: {determinant}\\ncondition: {condition}\\neigen mean: {np.mean(eigvals)}\\neigen variance: {np.var(eigvals)}')\n",
    "\n",
    "        determinant = np.linalg.det(self.reg_noise_cov_matrix) # should be greater than 10e-10\n",
    "        condition = np.linalg.cond(self.reg_noise_cov_matrix) # should be below 10e7\n",
    "\n",
    "        eigvals = np.linalg.eigvals(self.reg_noise_cov_matrix)\n",
    "\n",
    "        print(f'regularized matrix\\ndeterminant: {determinant}\\ncondition: {condition}\\neigen mean: {np.mean(eigvals)}\\neigen variance: {np.var(eigvals)}')\n",
    "\n",
    "    def check_noise_whitening(self):\n",
    "        whitened_noise_cov = self.whitening_matrix @ self.reg_noise_cov_matrix @ self.whitening_matrix.T\n",
    "\n",
    "        plt.imshow(whitened_noise_cov)  # if resembles identity matrix, whitening is applied correctly\n",
    "    \n",
    "    def plot_mnf_variance(self):\n",
    "        # plt.figure(figsize=(12, 6))\n",
    "        # for i in range(10):  \n",
    "        #     plt.subplot(2, 5, i + 1)\n",
    "        #     plt.imshow(self.mnf_image[i,:, :])\n",
    "        #     plt.title(f\"MNF Band {i+1}\")\n",
    "        # plt.show()\n",
    "\n",
    "        variance = [np.nanvar(self.mnf_image[i,:,:])for i in range(10)]\n",
    "\n",
    "        plt.plot(np.arange(0,len(variance)),variance)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "# get noise region based on variance\n",
    "def get_noise_region_variance(data,percentile):\n",
    "    \n",
    "    image = data[0:3,:,:]\n",
    "    #image = data\n",
    "    #apply window, calculate variance\n",
    "    variance_map = np.mean([generic_filter(image[b,:, :], lambda x: np.var(x), size=13) for b in range(image.shape[0])], axis=0)\n",
    "    # find areas with variance < percentile\n",
    "    x_2d = rearrange(variance_map,'h w -> (h w)')\n",
    "    varmap_dropna = x_2d[~np.isnan(x_2d)]   \n",
    "    threshold = np.percentile(varmap_dropna, percentile)\n",
    "    low_var = variance_map < threshold\n",
    "    #extract largest region of low variance\n",
    "    binary_mask = low_var.astype(np.uint8)\n",
    "\n",
    "    # Find contours of homogeneous regions\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Get the largest region\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    # Get bounding box coordinates\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "     #box(xmin, ymin, xmax, ymax,\n",
    "    bx = box(x,y+h,x+w,y)\n",
    "    \n",
    "    # Extract the subregion from the image\n",
    "    return data[:,y:y+h, x:x+w], bx\n",
    "\n",
    "# get noise region based on otsu thresholding\n",
    "def get_noise_region_otsu(data): \n",
    "    data_array = data.values\n",
    "    image_rearrange = rearrange(data_array,'c h w -> h w c' )\n",
    "    image = image_rearrange[:,:,0:3]\n",
    "    im = np.ma.masked_invalid(image)\n",
    "\n",
    "    im_norm = im / np.max(im) # normalize the data to 0 - 1\n",
    "    im_scale = 255 * im_norm # scale by 255\n",
    "    im_u8 = im_scale.astype(np.uint8) # convert to uint 8\n",
    "    img = cv2.cvtColor(im_u8, cv2.COLOR_BGR2GRAY) \n",
    "    # applying Otsu thresholding \n",
    "    # as an extra flag in binary  \n",
    "    # thresholding      \n",
    "    ret, low_var = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY + \n",
    "                                                cv2.THRESH_OTSU) \n",
    "\n",
    "    # extract largest region of low variance\n",
    "    binary_mask = low_var.astype(np.uint8)\n",
    "\n",
    "    # Find contours of homogeneous regions\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Get the largest region\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    # Get bounding box coordinates\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "     #box(xmin, ymin, xmax, ymax,\n",
    "    bx = box(x,y+h,x+w,y)\n",
    "    \n",
    "    # Extract the subregion from the image\n",
    "    return data[:,y:y+h, x:x+w], bx\n",
    "\n",
    "def estimate_noise(image):\n",
    "    noise = np.zeros_like(image)\n",
    "    \n",
    "    # Compute noise for each band\n",
    "    for b in range(image.shape[0]):\n",
    "        x_ij = image[b,:, :]\n",
    "        x_ip1_j = np.roll(x_ij, shift=-1, axis=0)  # Shift left (i+1, j)\n",
    "        x_i_jp1 = np.roll(x_ij, shift=-1, axis=1)  # Shift up (i, j+1)\n",
    "        \n",
    "        # Compute noise \n",
    "        noise[b, :, :] = x_ij - 0.5 * (x_ip1_j + x_i_jp1)\n",
    "    \n",
    "    return noise\n",
    "\n",
    "def estimate_noise_gaussian(img):\n",
    "  \n",
    "    # Generate Gaussian noise with mean=0, std=0.012 (same shape as image)\n",
    "    rnoisy = np.random.normal(0, 0.012, size=img.shape)\n",
    "\n",
    "    # Compute per-channel RMS (Root Mean Square) value\n",
    "    rms = np.sqrt(np.nanmean(img ** 2, axis=(1, 2), keepdims=True))\n",
    "\n",
    "    # Scale noise by image RMS \n",
    "    noise = rms * rnoisy\n",
    "\n",
    "    return noise\n",
    "\n",
    "def get_noise_cov_matrix(noise_matrix):\n",
    "    \n",
    "    noise = rearrange(noise_matrix, 'c h w -> (h w) c')\n",
    "\n",
    "    # remove nans\n",
    "    noise = noise[~np.isnan(noise).any(axis=1)]  # Remove rows with NaNs\n",
    "   \n",
    "    cov = np.cov(noise,rowvar=False)\n",
    "    \n",
    "    return cov\n",
    "\n",
    "def check_matrix(n_cov_matrix):\n",
    "# check stability of noise covariance matrix\n",
    "    determinant = np.linalg.det(n_cov_matrix) # should be greater than 10e-10\n",
    "    condition = np.linalg.cond(n_cov_matrix) # should be below 10e7\n",
    "\n",
    "    eigvals = np.linalg.eigvals(n_cov_matrix)\n",
    "\n",
    "    # data_variance = np.nanvar(data, axis=(1,2))  # Variance across spatial dimensions\n",
    "    # mean_data_variance = np.mean(data_variance)\n",
    "    # mean_data_variance\n",
    "\n",
    "    print(f'determinant: {determinant}\\ncondition: {condition}\\neigen min: {np.min(eigvals)}\\neigen max: {np.max(eigvals)}\\neigen mean: {np.mean(eigvals)}\\neigen variance: {np.var(eigvals)}\\neigvals: {eigvals}')\n",
    "\n",
    "####### adding regularization term gives more numerically resonable output ###########\n",
    "def regularize_noise(noise_cov_matrix):\n",
    "    lambda_reg = 1e-1 * np.eye(noise_cov_matrix.shape[0])  # diagonal offset\n",
    "    reg_cov_matrix = noise_cov_matrix + lambda_reg \n",
    "\n",
    "    return reg_cov_matrix\n",
    "\n",
    "\n",
    "def get_ba_threshold(basal_area):\n",
    "    r = basal_area.values\n",
    "    r = r[~np.isnan(r)]\n",
    "\n",
    "    return round(np.percentile(r,99),3)\n",
    "\n",
    "def get_endmembers(ba,mnf,ts):  # return pixels with > 99th percentile of basal area\n",
    "    #thresh = get_ba_threshold(ba)\n",
    "    h = ba\n",
    "    #rows, cols = np.where(h>=thresh)\n",
    "    rows, cols = np.where(h==100.0)\n",
    "    pixels = [(int(row),int(col)) for row,col in zip(rows,cols)]\n",
    "    print(f'found {len(pixels)} pixels where {ts} is 100 percent basal area')\n",
    "  \n",
    "\n",
    "    p_list = []\n",
    "\n",
    "    for i in range(0,len(pixels)):\n",
    "        p_list.append(mnf.values[:,pixels[i][0],pixels[i][1]])\n",
    "\n",
    "    p_array = np.array(p_list) \n",
    "    \n",
    "\n",
    "    p_mean = np.nanmean(p_array,axis=0)\n",
    "\n",
    "    return p_array, p_mean\n",
    "\n",
    "def get_endmembers_manual(em, mnf):\n",
    "    r = em.get_coordinates()\n",
    "\n",
    "    v = []\n",
    "    for i in range(r.shape[0]):\n",
    "        value = mnf.sel(x=r.iloc[i,0], y=r.iloc[i,1], method=\"nearest\").values\n",
    "        v.append(value)\n",
    "\n",
    "    v_mean = np.mean(v,axis=0)\n",
    "\n",
    "    return v, v_mean\n",
    "    \n",
    "def create_mnf_image(datastack,n1,n2,reg=True):\n",
    "    # takes in DataStack object\n",
    "    # outputs MNF object with:\n",
    "        # noise region bbox\n",
    "        # noise covariance matrix\n",
    "        # mnf image\n",
    "        # endmembers in mnf space\n",
    "    data = datastack.mnf_input.values[n1:n2,:,:]\n",
    "    \n",
    "\n",
    "    noise_region, noise_bbox = get_noise_region_variance(data, 30) # first three bands only\n",
    "\n",
    "    noise = estimate_noise_gaussian(noise_region)\n",
    "    ############ step 1: whiten data ###########\n",
    "    n_cov_matrix = get_noise_cov_matrix(noise)\n",
    "    if reg == True:\n",
    "        reg_n_cov_matrix = regularize_noise(n_cov_matrix)\n",
    "        eigvals, eigvecs = np.linalg.eigh(reg_n_cov_matrix)\n",
    "    else:\n",
    "        eigvals, eigvecs = np.linalg.eigh(n_cov_matrix)\n",
    "        reg_n_cov_matrix = None\n",
    "\n",
    "\n",
    "    # mean correct and reshape the data \n",
    "    m = np.nanmean(data, axis=0)\n",
    "    data = data - m\n",
    "    reshaped_data = rearrange(data,'b h w -> b (h w)') \n",
    "\n",
    "    # Compute the whitening matrix \n",
    "    inv_sqrt_eigenvalues = np.diag(1.0 / np.sqrt(eigvals))  # Compute R = Λ^(-1/2)\n",
    "    whitening_matrix = eigvecs @ inv_sqrt_eigenvalues @ eigvecs.T  \\\n",
    "    # apply whitening transform\n",
    "    whitened_data = whitening_matrix @ reshaped_data\n",
    "\n",
    "    ####### step 2: PCA rotation #######\n",
    "    # drop nas before applying pca\n",
    "    whitened_data = rearrange(whitened_data,'c r -> r c')\n",
    "    dropna = whitened_data[~np.isnan(whitened_data).any(axis=1)]\n",
    "    # get indices of dropped nas to replace them later\n",
    "    na_indices = np.where(np.isnan(whitened_data))\n",
    "    row_indices = np.unique(na_indices[0]) # indices of rows that were dropped \n",
    "\n",
    "    # pca on whitened data; needs to be in form (rows, columns)\n",
    "    pca = PCA()\n",
    "    mnf = pca.fit_transform(dropna)\n",
    "\n",
    "    # replace nans that were dropped earlier\n",
    "    output_array = np.full((whitened_data.shape[0],mnf.shape[1]), np.nan)  # create array of correct shape\n",
    "    non_nan_indices = np.setdiff1d(np.arange(whitened_data.shape[0]), row_indices) \n",
    "    output_array[non_nan_indices] = mnf\n",
    "\n",
    "    # Reshape back to image format\n",
    "    mnf_image = rearrange(output_array,'(h w) b -> b h w',h=data.shape[1],w=data.shape[2])\n",
    "\n",
    "\n",
    "    # convert to xarray with geographic coodinates\n",
    "    band_names = [f'comp_{x}' for x in range(mnf_image.shape[0])]\n",
    "    mnf_image = xr.DataArray(mnf_image, coords={'band': band_names,'y': datastack.y_coords, 'x': datastack.x_coords},dims=['band','y','x'])\n",
    "    \n",
    "    ########### get endmembers ###############\n",
    "    # if em_manual == True:\n",
    "    #     endmember_array, endmember_mean = get_endmembers_manual(datastack.endmember_manual, mnf=mnf_image)\n",
    "    # else:\n",
    "    #     endmember_array, endmember_mean = get_endmembers(datastack.basal_area, mnf_image)\n",
    "\n",
    "    return MNF(mnf_image = mnf_image,noise_bbox=noise_bbox, noise_cov_matrix=n_cov_matrix, reg_noise_cov_matrix=reg_n_cov_matrix,whitening_matrix=whitening_matrix,basal_area=datastack.basal_area,labels=datastack.labels,root=datastack.root,site_name=datastack.site_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get cov matrix of mnf, find how many eigenvalues are > threshold, select that number of components from mnf image\n",
    "def get_num_high_signal_components(mnf_image,thresh):\n",
    "    mnf_2d = rearrange(mnf_image, 'c h w -> (h w) c')\n",
    "    mnf_dropna = mnf_2d[~np.isnan(mnf_2d).any(axis=1)]  # Remove rows with NaNs\n",
    "    mnf_cov = np.cov(mnf_dropna,rowvar=False)\n",
    "    mnf_eigvals, _ = np.linalg.eigh(mnf_cov)\n",
    "    return len(mnf_eigvals[mnf_eigvals>=thresh]), mnf_eigvals\n",
    "\n",
    "def make_2d_input_data(data):\n",
    "    x_2d = rearrange(data,'c h w -> (h w) c')\n",
    "    x_dropna = x_2d[~np.isnan(x_2d).any(axis=1)]  # Remove rows with NaNs\n",
    "    \n",
    "    # get indices of dropped nas to replace them later\n",
    "    na_indices = np.where(np.isnan(x_2d))\n",
    "    row_indices = np.unique(na_indices[0]) # indices of rows that were dropped from a_2d\n",
    "\n",
    "    return x_2d, x_dropna, row_indices\n",
    "\n",
    "def replace_na_and_reshape(data_2d,final_data,indices,shape):\n",
    "# replace nans that were dropped earlier\n",
    "    output = np.full((data_2d.shape[0]), np.nan)  # create array of correct shape\n",
    "\n",
    "    # Fill in the rows that were not NaN\n",
    "    non_nan_indices = np.setdiff1d(np.arange(data_2d.shape[0]), indices)  # get indices that don't have na values\n",
    "    output[non_nan_indices] = final_data\n",
    "\n",
    "    r = output.reshape((shape[1],shape[2]) + (-1,)).squeeze()\n",
    "    return r\n",
    "    \n",
    "\n",
    "\n",
    "def mixture_tuned_matched_filter(mnf_object,SNR,mean=True,array_num=0,target_species='beech'):\n",
    "    # takes in MNF object\n",
    "    # outputs array with MF score, IF score, and basal area bands\n",
    "\n",
    "    # add get endmembers here\n",
    "    # mnf_object.basal_area[f'{target_species}_basal_area']\n",
    "    endmember_array, endmember_mean = get_endmembers(mnf_object.basal_area[f'{target_species}_basal_area'], mnf_object.mnf_image,ts=target_species)\n",
    "\n",
    "\n",
    "    ###### select MNF components to use###########\n",
    "    t = SNR # threshold for eigenvalues\n",
    "    n, eigvals = get_num_high_signal_components(mnf_object.mnf_image.values,t)\n",
    "    mnf = mnf_object.mnf_image.values[:n,:,:]\n",
    "    print(f'{n} MNF components used with eigvals between {np.min(eigvals[eigvals>=t])} and {np.max(eigvals[eigvals>=t])}')\n",
    "\n",
    "    # Target spectral signature \n",
    "    if mean == True:\n",
    "        #target = mnf_object.em_mean[:n]\n",
    "        target = endmember_mean[:n]\n",
    "    else:\n",
    "        #target = mnf_object.em_array[array_num][:n]\n",
    "        target = endmember_array[array_num][:n]\n",
    "    # background mean \n",
    "    #u_b = np.nanmean(mnf, axis=(1, 2))  \n",
    "    # difference between target and background mean - not present in Mundt\n",
    "    #d_tb = (target- u_b)\n",
    "    ############ matched filter ######################\n",
    "    # Calculate the covariance matrix of the background\n",
    "    mnf_2d, mnf_dropna, na_indices = make_2d_input_data(mnf) \n",
    "    C = np.cov(mnf_dropna, rowvar=False)  \n",
    "\n",
    "    det = np.linalg.det(C) # should be greater than 10e-10\n",
    "    cond = np.linalg.cond(C) # should be below 10e7\n",
    "    print(f'mnf covariance matrix:\\n  determinant: {det}\\n  condition: {cond}')\n",
    "    C1 = np.linalg.inv(C)\n",
    "    #epsilon = 1e-6  # Small regularization term\n",
    "    #C1 = np.linalg.inv(C + epsilon * np.eye(C.shape[0]))\n",
    "\n",
    "    # Normalization coefficient \n",
    "    coef = 1.0 / target @ C1 @ target\n",
    "    \n",
    "    A = (coef * target) @ C1 # transformation matrix\n",
    "    Y = (A @ mnf_dropna.T).T\n",
    "\n",
    "    matched_filter = replace_na_and_reshape(mnf_2d,Y,na_indices,mnf.shape)\n",
    "\n",
    "    print(f'MF max: {np.nanmax(matched_filter)}, MF min: {np.nanmin(matched_filter)}, MF mean: {np.nanmean(matched_filter)}')\n",
    "\n",
    "    \n",
    "\n",
    "    ######## infeasibility score #################\n",
    "\n",
    "    # Compute target vector component \n",
    "    target_component = matched_filter[..., np.newaxis] * target  # Shape: (H, W, D)\n",
    "    target_component = rearrange(target_component,'h w d -> d h w')\n",
    "\n",
    "    eigvals, _ = np.linalg.eigh(C) # C = covariance matrix of mnf image\n",
    "    # Compute interpolated eigenvalues; scale mnf eigenvalues based on the MF scores\n",
    "    noise = np.ones((n,))  # size corresponds to number of components; noise has unit variance\n",
    "\n",
    "    sqrt_eig = np.sqrt(eigvals)  \n",
    "    interp_eig = (sqrt_eig - matched_filter[..., np.newaxis] * (sqrt_eig - noise))**2  \n",
    "\n",
    "    interp_eig = np.where(interp_eig == 0, 1e-6, interp_eig)  # Avoid divide-by-zero issues\n",
    "    interp_eig = rearrange(interp_eig,'h w d -> d h w')\n",
    "\n",
    "    r = mnf - target_component\n",
    "    r_2d, r_dropna, r_indices = make_2d_input_data(r)\n",
    "\n",
    "    r_norm = np.linalg.norm(np.nan_to_num(r_dropna),axis=1)\n",
    "    r2 = replace_na_and_reshape(r_2d,r_norm,r_indices,mnf.shape)\n",
    "\n",
    "    e_2d, e_dropna, e_indices = make_2d_input_data(interp_eig)\n",
    "\n",
    "    s = np.linalg.norm(np.nan_to_num(e_dropna,nan=1e-6),axis=1)\n",
    "    s2 = replace_na_and_reshape(e_2d,s,e_indices,mnf.shape)\n",
    "\n",
    "    infeas_score = r2/s2\n",
    "\n",
    "    print(f'infeas max: {np.nanmax(infeas_score)}, infeas min: {np.nanmin(infeas_score)}, infeas mean: {np.nanmean(infeas_score)}')\n",
    "\n",
    "    matched_filter = xr.DataArray(matched_filter, coords={'y': mnf_object.mnf_image.y.values, 'x': mnf_object.mnf_image.x.values},dims=['y','x'])\n",
    "\n",
    "    infeas_score = xr.DataArray(infeas_score, coords={'y': mnf_object.mnf_image.y.values, 'x': mnf_object.mnf_image.x.values},dims=['y','x'])\n",
    "\n",
    "    q = xr.concat([matched_filter,infeas_score,mnf_object.basal_area[f'{target_species}_basal_area'],mnf_object.basal_area[f'{target_species}_category']],dim='band')\n",
    "    q = q.assign_coords({'band':[f'{target_species}_mf_score',f'{target_species}_infeas_score',f'{target_species}_basal_area',f'{target_species}_categories']})\n",
    "    \n",
    "    return MTMF(array=q,target=target,target_species=target_species,labels=mnf_object.labels,root=mnf_object.root,site_name=mnf_object.site_name)\n",
    "\n",
    "\n",
    "\n",
    "class MTMF():\n",
    "    def __init__(self,array,target,target_species,labels,root,site_name):\n",
    "        self.result_array = array\n",
    "        self.target = target\n",
    "        self.matched_filter = self.result_array.sel(band=f'{target_species}_mf_score').values\n",
    "        self.infeas_score = self.result_array.sel(band=f'{target_species}_infeas_score').values\n",
    "        self.basal_area = self.result_array.sel(band=f'{target_species}_basal_area').values\n",
    "        self.categories = self.result_array.sel(band=f'{target_species}_categories').values\n",
    "        self.labels = labels\n",
    "        self.target_species = target_species\n",
    "        self.root = root\n",
    "        self.site_name = site_name\n",
    "\n",
    "\n",
    "    def plot_histograms(self):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "        ax[0].hist(self.matched_filter.flatten(), bins=50)\n",
    "        ax[0].set_title(\"Matched Filter\")\n",
    "\n",
    "        ax[1].hist(self.infeas_score.flatten(), bins=50)\n",
    "        ax[1].set_title(\"Infeasibility\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_correlation(self):\n",
    "        \n",
    "        i_2d = rearrange(self.infeas_score,'h w -> (h w)')\n",
    "        ba_2d = rearrange(self.basal_area,'h w -> (h w)')\n",
    "        t2_2d = rearrange(self.matched_filter,'h w -> (h w)')\n",
    "        c_2d = rearrange(self.categories,'h w -> (h w)' )\n",
    "\n",
    "        combine = np.vstack((ba_2d,t2_2d,i_2d,c_2d)).T\n",
    "\n",
    "        combine = combine[~np.isnan(combine).any(axis=1)]\n",
    "\n",
    "        df = pd.DataFrame(combine)\n",
    "        df.columns = ['basal_area','mf_score','infeas_score','category']\n",
    "        # read in category labels for target species\n",
    "        label_df = pd.read_csv([x for x in self.labels if self.target_species in x][0])\n",
    "\n",
    "        # make dictionary to make sure labels are properly matched to categories\n",
    "        label_dict = {'0.0':label_df.loc[0,'cat_labels'],'1.0':label_df.loc[1,'cat_labels'],\n",
    "              '2.0':label_df.loc[2,'cat_labels'],'3.0':label_df.loc[3,'cat_labels'],\n",
    "              '4.0':label_df.loc[4,'cat_labels'],'5.0':label_df.loc[5,'cat_labels'],\n",
    "              '6.0':label_df.loc[6,'cat_labels'],'7.0':label_df.loc[7,'cat_labels']}\n",
    "        \n",
    "\n",
    "        g = sns.scatterplot(data=df, x='mf_score', y='infeas_score', hue='category',palette=sns.color_palette('tab10'))\n",
    "        handles, labels = g.get_legend_handles_labels()\n",
    "        g.legend(handles=handles,labels=[label_dict.get(x) for x in labels])\n",
    "        sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    def plot_threshold(self,mf_greaterthan,infeas_lessthan,save=False,epsg=None):\n",
    "        mask_1 = self.result_array.sel(band='mf_score')>mf_greaterthan\n",
    "        mask_2 = self.result_array.sel(band='infeas_score')<infeas_lessthan\n",
    "\n",
    "        threshold = self.result_array.where(mask_1 & mask_2)\n",
    "        threshold.sel(band='mf_score').plot() \n",
    "        \n",
    "        if save == True:\n",
    "            threshold = threshold.rio.write_crs(epsg).rio.set_spatial_dims(x_dim=\"x\",y_dim=\"y\",).rio.write_coordinate_system()\n",
    "            threshold.sel(band='mf_score').to_netcdf(self.root / 'output' / self.site_name.upper() / f'{self.target_species}_mtmf_result.nc')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def make_training_df(m):\n",
    "    # take mtmf object or list of mtmf objects\n",
    "    if type(m) == list:\n",
    "        m = [x.result_array for x in m]\n",
    "        m = xr.concat(m,dim='band')\n",
    "    else:\n",
    "        m = m.result_array\n",
    "\n",
    "    out_df = pd.DataFrame(rearrange(m.values,'b h w -> (h w) b'),columns=m.band.values)\n",
    "    out_df = out_df[~np.isnan(out_df).any(axis=1)]\n",
    "    \n",
    "    return out_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = MnfInput object (stack of bands to use to calculate mnf)\n",
    "# months = which months bands to include\n",
    "# v1, v2 = which version of pca to use\n",
    "# diff_desc = which difference to use (e.g. 'summerminusfall')\n",
    "\n",
    "#gs_tc_mnf_input = roi1.make_mnf_input(v1='gs',v2='tc',month='month2',diffs=True,diff_num=0)\n",
    "bart_data.make_mnf_input(v1='ssf',v2='tc',month=['month2'],diff_desc='summerminusfall')\n",
    "#ssf_tc_mnf_input = roi1.make_mnf_input(v1='ssf',v2='tc',month='month2',diffs=True,diff_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseh\\AppData\\Local\\Temp\\ipykernel_15452\\4017859603.py:239: RuntimeWarning: Mean of empty slice\n",
      "  m = np.nanmean(data, axis=0)\n"
     ]
    }
   ],
   "source": [
    "bart_mnf = create_mnf_image(bart_data,n1=0,n2=28,reg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 16 pixels where beech is 100 percent basal area\n",
      "3 MNF components used with eigvals between 41.725003265712516 and 224.1321456589222\n",
      "mnf covariance matrix:\n",
      "  determinant: 475380.2680401698\n",
      "  condition: 5.371650763730503\n",
      "MF max: 0.730934895850299, MF min: -0.3065107042279011, MF mean: 3.0559982118644955e-17\n",
      "infeas max: 4.962578476184553, infeas min: 0.0006447365972009959, infeas mean: 0.0670604557925415\n"
     ]
    }
   ],
   "source": [
    "bart_mtmf = mixture_tuned_matched_filter(bart_mnf,SNR=28,mean=True,array_num=0,target_species='beech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tuning multiple algorithms\n",
    "# xgboost\n",
    "# random forest\n",
    "# gbm\n",
    "# \n",
    "# models = {\n",
    "#     \"Random Forest\": RandomForestRegressor(\n",
    "#         min_samples_leaf=5, random_state=0, n_jobs=N_CORES\n",
    "#     ),\n",
    "#     \"Hist Gradient Boosting\": HistGradientBoostingRegressor(\n",
    "#         max_leaf_nodes=15, random_state=0, early_stopping=False\n",
    "#     ),\n",
    "# }\n",
    "# param_grids = {\n",
    "#     \"Random Forest\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "#     \"Hist Gradient Boosting\": {\"max_iter\": [10, 20, 50, 100, 300, 500]},\n",
    "# }\n",
    "# cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# results = []\n",
    "# for name, model in models.items():\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=model,\n",
    "#         param_grid=param_grids[name],\n",
    "#         return_train_score=True,\n",
    "#         cv=cv,\n",
    "#     ).fit(X, y)\n",
    "#     result = {\"model\": name, \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\n",
    "#     results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beech_df = make_training_df([bart_mtmf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using sklearn.ensemble.StackingRegressor after regressors are all optimized\n",
    "# or sklearn.multioutput.RegressorChain\n",
    "\n",
    "def tune_regression_models(df,target_species,param_grids):\n",
    "\n",
    "    # make training data\n",
    "    ind_vars = [x for x in df.columns if any(y in x for y in ['mf_score','infeas_score'])]\n",
    "    X = df.loc[:,ind_vars].values\n",
    "    y = df.loc[:,f'{target_species}_basal_area'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\n",
    "\n",
    "    models = {\n",
    "        \"mlp\": MLPRegressor(),\n",
    "        \"xgboost\": xgb.XGBRegressor(),\n",
    "        \"gbm\": GradientBoostingRegressor(),\n",
    "        'randomforest': RandomForestRegressor()\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        grid_search = BayesSearchCV(\n",
    "            estimator=model,\n",
    "            search_spaces=param_grids[name],\n",
    "            return_train_score=True,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            cv=cv\n",
    "            ).fit(X_train, y_train)\n",
    "\n",
    "        param_dict = grid_search.best_params_\n",
    "        param_dict['model'] = name\n",
    "        param_dict['test_rmse'] = (np.abs(grid_search.score(X_test,y_test)))\n",
    "        param_dict['train_rmse'] = (np.abs(grid_search.best_score_))\n",
    "        results.append(pd.DataFrame(param_dict,index=[param_dict['model']]).drop(columns='model'))\n",
    "\n",
    "    return pd.concat(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param_grids = {\n",
    "    \"mlp\":          {\n",
    "                    'hidden_layer_sizes': Integer(100,200), \n",
    "                    'activation':Categorical(['relu']), \n",
    "                    'solver':Categorical(['lbfgs']), # lbfgs more suited to smaller datasets\n",
    "                    'alpha': Real(0.0001,0.1,'log-uniform'),  \n",
    "                    'max_iter':Integer(10000,50000),  \n",
    "                    'max_fun':Integer(10000,15000)\n",
    "                    },\n",
    "\n",
    "    \"randomforest\": {\"n_estimators\": Integer(100,200),\n",
    "                     \"min_samples_split\": Integer(20,50),\n",
    "                     'min_samples_leaf': Integer(20,50),\n",
    "                     'ccp_alpha':Real(1e-6,0.1,'log-uniform')},\n",
    "\n",
    "    \"xgboost\":      {'max_depth': Integer(1,5),\n",
    "                    'learning_rate':Real(1e-6,0.1,'log-uniform'),\n",
    "                    'gamma': Real(1e-6,0.1, 'log-uniform'),  # minimum loss reduction required to make another split (gain)\n",
    "                    'reg_lambda': Real(1e-6,0.2,'log-uniform'), # L2 regularization of weights\n",
    "                    'n_estimators':Integer(50,200)\n",
    "                    #'early_stopping_rounds':[10]\n",
    "                    },\n",
    "\n",
    "    'gbm':          {'learning_rate':Real(0.3,1.0,'log-uniform'),  # factor by which to shrink the contribution of each tree \n",
    "                    'n_estimators':Integer(500,2000),\n",
    "                    'max_depth': Integer(1,10),\n",
    "                    'n_iter_no_change': Integer(1,30),\n",
    "                    'validation_fraction':Real(0.01,0.1,'log-uniform'),\n",
    "                    'ccp_alpha':Real(0.01,1.0,'log-uniform')}  # stops pruning when a tree's minimimal cost-complexity is greater than ccp_alpha\n",
    "}\n",
    "\n",
    "result_round3 = tune_regression_models(df=beech_df,target_species='beech',param_grids=reg_param_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>max_fun</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>solver</th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>ccp_alpha</th>\n",
       "      <th>n_iter_no_change</th>\n",
       "      <th>validation_fraction</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>111.0</td>\n",
       "      <td>13946.0</td>\n",
       "      <td>49028.0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>29.954715</td>\n",
       "      <td>24.659732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.033457</td>\n",
       "      <td>24.719835</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.080288</td>\n",
       "      <td>24.747381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.476945</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.062481</td>\n",
       "      <td>24.689953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             activation  alpha  hidden_layer_sizes  max_fun  max_iter solver  \\\n",
       "mlp                relu    0.1               111.0  13946.0   49028.0  lbfgs   \n",
       "xgboost             NaN    NaN                 NaN      NaN       NaN    NaN   \n",
       "gbm                 NaN    NaN                 NaN      NaN       NaN    NaN   \n",
       "randomforest        NaN    NaN                 NaN      NaN       NaN    NaN   \n",
       "\n",
       "              test_rmse  train_rmse     gamma  learning_rate  max_depth  \\\n",
       "mlp           29.954715   24.659732       NaN            NaN        NaN   \n",
       "xgboost       30.033457   24.719835  0.000008       0.086059        1.0   \n",
       "gbm           30.080288   24.747381       NaN       0.476945        1.0   \n",
       "randomforest  30.062481   24.689953       NaN            NaN        NaN   \n",
       "\n",
       "              n_estimators  reg_lambda  ccp_alpha  n_iter_no_change  \\\n",
       "mlp                    NaN         NaN        NaN               NaN   \n",
       "xgboost               50.0    0.000001        NaN               NaN   \n",
       "gbm                 2000.0         NaN   1.000000               1.0   \n",
       "randomforest         185.0         NaN   0.000068               NaN   \n",
       "\n",
       "              validation_fraction  min_samples_leaf  min_samples_split  \n",
       "mlp                           NaN               NaN                NaN  \n",
       "xgboost                       NaN               NaN                NaN  \n",
       "gbm                           0.1               NaN                NaN  \n",
       "randomforest                  NaN              50.0               20.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_round3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['beech_mf_score', 'beech_infeas_score', 'beech_basal_area',\n",
       "       'beech_categories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recategorize to 0%, 100%, >50%, <50%\n",
    "\n",
    "# pipeline: TargetEncoder()\n",
    "def add_category_columns(df,target_species):\n",
    "\n",
    "    conditions_binary = [(df[f'{target_species}_basal_area']==0.0),\n",
    "                df[f'{target_species}_basal_area']>0.0\n",
    "                ]\n",
    "    binary_cats = [0,1]\n",
    "\n",
    "    conditions_3cats = [(df[f'{target_species}_basal_area']==0.0),\n",
    "                (df[f'{target_species}_basal_area']>0.0)&(df[f'{target_species}_basal_area']<50.0),\n",
    "                #(df['basal_area']>=50.0)&(df['basal_area']<=100.0)\n",
    "                df[f'{target_species}_basal_area']>=50.0\n",
    "                ]\n",
    "    three_cats = [0,1,2]\n",
    "\n",
    "    df['three_categories'] = np.select(conditions_3cats,three_cats, default=0)\n",
    "    df['binary'] = np.select(conditions_binary,binary_cats, default=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "#clf = xgb.XGBClassifier(objective='multi:softmax',eval_metric='auc',early_stopping_rounds=10)  #\"multi:softprob\"\n",
    "\n",
    "beech_df = add_category_columns(beech_df,'beech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "beech_df.to_csv(root / 'output' / 'BART' / 'mtmf_df_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(25.104306319202443)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = regr.predict(X_test)\n",
    "#confusion_matrix(y_true=y_test,y_pred=y_pred)\n",
    "root_mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Marconi: stacked classifiers\n",
    "\n",
    "\n",
    "#define models\n",
    "rf2 = make_pipeline(StandardScaler(),\n",
    "                    RandomForestClassifier(random_state=0, oob_score = True,\n",
    "        n_estimators = 300, max_features = 'sqrt', criterion = 'entropy'))\n",
    "\n",
    "\n",
    "knn2 = make_pipeline(StandardScaler(),\n",
    "                     KNeighborsClassifier(weights = 'distance', \n",
    "                                          p=1, n_neighbors=20))\n",
    "gb2 = make_pipeline(StandardScaler(),\n",
    "                    HistGradientBoostingClassifier(random_state=0, \n",
    "                    max_iter = 1000, learning_rate = 0.01, \n",
    "                max_depth = 25, loss = 'categorical_crossentropy', \n",
    "                l2_regularization = 0.5))\n",
    "mlpc2 = make_pipeline(StandardScaler(), \n",
    "                      MLPClassifier(random_state=0, \n",
    "                                    beta_2=0.9, max_iter = 1200))\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "bayes2 = make_pipeline(StandardScaler(), GaussianNB())\n",
    "\n",
    "bsvc2 =make_pipeline(StandardScaler(),\n",
    "                     BaggingClassifier(\n",
    "                         base_estimator=SVC(probability = True, C = 1000), \n",
    "                        n_jobs = 1, random_state=0))\n",
    "\n",
    "logc = LogisticRegression(penalty = \"elasticnet\", solver = \"saga\", \n",
    "                          max_iter = 10000, n_jobs=3, l1_ratio = 0.5)\n",
    "\n",
    "\n",
    "logc = LogisticRegression(penalty = \"elasticnet\", solver = \"saga\", \n",
    "                          max_iter = 10000, n_jobs=3, l1_ratio = 0.5)\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "clf_bl2 = StackingCVClassifier(classifiers = [rf2, gb2, bsvc2, mlpc2, knn2],\n",
    "            use_probas=True, cv = 3, n_jobs =1,\n",
    "            meta_classifier= logc)\n",
    "\n",
    "clf_bl2.fit(X_res, y_res.taxonID.ravel())\n",
    "print(clf_bl2.score(X_test, y_test['taxonID'].ravel()))\n",
    "\n",
    "predict_an = clf_bl2.predict_proba(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beech-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
